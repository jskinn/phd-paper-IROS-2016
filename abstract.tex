% !TEX root = root.tex

% TODO: Re-write after the rest of the paper
Robotic vision, unlike computer vision, typically involves processing a stream of images from a camera with time varying pose operating in an environment with
time varying lighting conditions and moving objects. Repeating robotic vision experiments under identical conditions is often impossible, making it difficult to compare different algorithms. For machine learning applications a critical bottleneck is the limited amount of real world image data that can be captured and labelled for both training and testing purposes. 
In this paper we investigate the use of a photo-realistic  simulation tool to address these challenges, in three specific domains: robust place recognition, visual SLAM and object recognition. 
For the first two problems we generate generate images from a complex 3D environment with systematically varying camera paths, camera viewpoints and lighting conditions.
For the first time we are able to systematically characterize the performance of these algorithms as paths and lighting conditions change. In particular, we are able to systematically generate varying camera viewpoint datasets that would be difficult or impossible to generate in the real world. We also compare algorithm results for a camera in a  real environment and a simulated camera in a simulation model of that real environment.
Finally, for the object recognition domain, we generate labelled image data and characterise the viewpoint dependency of a current convolution neural network in performing object recognition.
Together these results provide a multi-domain demonstration of the beneficial properties of using simulation to characterize and analyse a wide range of robotic vision algorithms.


%In this paper, we investigate how these limitations can be addressed by using a high-fidelity simulation environment to generate image datasets. To achieve this, we use an existing state of the art simulation tool, Unreal Engine 4 by Epic Games, to generate a variety of image datasets that contain task-relevant variation including camera-viewpoint change and time of day changes, and demonstrate that algorithm performance in the real world is similar to in simulation. These datasets enable us to systematically evaluate current state-of-the-art place recognition algorithms at a granularity that was previously impossible, both in terms of comparing algorithms to each other but also evaluating the effect of varying key algorithm parameters. Together, this work opens up new opportunities for development and analysis of algorithms, without the added cost of capturing real-world datasets with which to test the algorithm under development.
%
%In domains such as robot navigation, certain types of highly relevant data is difficult and labor intensive to obtain - for example, multiple traverses of a road under all possible environmental conditions or from all possible camera viewpoints.