%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{gensymb}
\usepackage{graphicx}
\usepackage{import}
\graphicspath{ {images/} }
\maxdeadcycles=1000
\setlength{\tabcolsep}{3pt}

\title{\LARGE \bf
High-Fidelity Simulation for Evaluating Robotic Vision Performance
}


\author{John Skinner$^{1}$, Sourav Garg$^{1}$, Niko S\"{u}nderhauf$^{1}$, Peter Corke$^{1}$, Ben Upcroft$^{1}$, Michael Milford$^{1}$% <-this % stops a space
\thanks{$^{1}$ Australian Centre for Robotic Vision,
        Queensland University of Technology,
        2 George St, Brisbane, Australia}%
%        {\tt\small jr.skinner@hdr.qut.edu.au}
%        {\tt\small sourav.garg@hdr.qut.edu.au}
%        {\tt\small niko.suenderhauf@qut.edu.au}
%        {\tt\small peter.corke@qut.edu.au}
%        {\tt\small ben.upcroft@qut.edu.au}
%        {\tt\small michael.milford@qut.edu.au}
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

\import{./}{abstract.tex}

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION} \label{sec:introduction}

\import{./}{introduction.tex}

\begin{figure*}[t]
    \centering
    \includegraphics[height=8cm]{street-scene-overview-dataset-examples}
    \caption{The custom street scene used to capture the image datasets that were used for testing the place recognition algorithms. The line of white dots shows the baseline path followed by the camera when generating the datasets to test Sum of Absolute Differences  and SeqSLAM place recognition performance. The SLAM experiments followed a slightly different path in order to ensure a complete loop of the environment.}
    \label{fig:street-scene-overview}
\end{figure*}

\section{PRIOR RESEARCH} \label{sec:prior-research}

In this section we review the prior usage of simulation in robotics research and the two primary domains of place recognition and visual SLAM we use to investigate the utility of simulation.

\import{./}{prior-research.tex}

\section{ANALYSIS OF PLACE RECOGNITION} \label{sec:analysis-place-rec}

\begin{figure*}[t]
    \centering
    \includegraphics[width=13cm]{data-comparison-stacked}
    \caption{A Sample of the variation found in the datasets used for testing. The first row shows lateral offset, from left to right, images are offset are left 3.5m, 2m, 1m, 0.4m, 0.2m, Then the baseline, then offset right 0.2m, 0.4m, 1m, 2m, and 3.5m. The second row shows vertical orientation change,  left to right, images are angled are up $30\degree$, $15\degree$, $10\degree$, $5\degree$, Then the baseline, then angled down $5\degree$, $10\degree$, $15\degree$, and $30\degree$. The third row shows horizontal orientation change, from left to right, images are angled are left $30\degree$, $15\degree$, $10\degree$, $5\degree$, Then the baseline, then angled right$5\degree$, $10\degree$, $15\degree$, and $30\degree$. Finally, the lowest row shows samples of time of day variation, from left to right, images are taken at dawn, in the morning, at noon, in the afternoon, and at sunset.}
    \label{fig:dataset-example}
\end{figure*}

%\begin{figure*}[t]
%    \includegraphics[width=16cm,height=2cm]{lateral-offset-comparison}
%    \caption{Sample images used to test lateral offset viewpoint invariance. From left to right, images are offset are left $3.5m$, $2m$, $1m$, $0.4m$, $0.2m$, Then the baseline, then offset right $0.2m$, $0.4m$, $1m$, $2m$, and $3.5m$}
%    \label{fig:dataset-offset-example}
%\end{figure*}
%
%\begin{figure*}[t]
%    \includegraphics[width=16cm]{vertical-orientation-change-comparison}
%    \caption{Sample images used to test orientation viewpoint invariance. From left to right, images are angled are up $30\degree$, $15\degree$, $10\degree$, $5\degree$, Then the baseline, then angled down $5\degree$, $10\degree$, $15\degree$, and $30\degree$}
%    \label{fig:dataset-pitch-example}
%\end{figure*}
%
%\begin{figure*}[t]
%    \includegraphics[width=16cm,height=2cm]{horizontal-orientation-change-comparison}
%    \caption{Sample images used to test orientation viewpoint invariance. From left to right, images are angled are left $30\degree$, $15\degree$, $10\degree$, $5\degree$, Then the baseline, then angled right$5\degree$, $10\degree$, $15\degree$, and $30\degree$}
%    \label{fig:dataset-yaw-example}
%\end{figure*}
%
%\begin{figure*}[t]
%    \includegraphics[width=16cm,height=2cm]{time-of-day-comparison}
%    \caption{Sample images used to test condition invariance. From left to right, images are taken at dawn, in the morning, at noon, in the afternoon, and at sunset}
%    \label{fig:dataset-time-of-day-example}
%\end{figure*}

To demonstrate the effectiveness of high-fidelity simulation as an analysis tool for robotic vision, we performed an in depth analysis of the viewpoint and time-of-day invariance of two different state-of-the-art place recognition algorithms, SeqSLAM \cite{Milford2012} and OrbSLAM \cite{Montiel2015}, and the viewpoint sensitivity of a state-of-the-art object recognition system.

\subsection{Simulation Setup: Unreal Engine 4}

The simulation tool used in this paper was the Unreal Engine 4, developed by Epic Games Inc. All of the test images used were generated from the same street scene, shown in Figure \ref{fig:street-scene-overview}. The 3D models used were either sourced for free from TurboSquid (www.turbosquid.com), with significant manual clean up, or were produced manually. The landscape was produced using the basic version of WorldMachine (http://www.world-machine.com). All lighting is as computed by the Unreal Engine, using standard sky and light assets provided with the engine.

To generate the image data, the camera was moved along a specified path to produce images at fixed spatial intervals.
The simulator allows us to precisely repeat the same path and introduce calculated and precise variations upon it.
In order to explore how the performance of place recognition changes with time of day and viewpoint change, passes along the path were generated for 5 different times of day in combination with increasing lateral offsets or camera tilts (Figure \ref{fig:dataset-example}).

%\begin{figure}[t]
%    \includegraphics[width=8cm]{dataset-image-variety-grid}
%    \caption{Sample images from the datasets used, all images are taken from the same place at different times of day and viewpoint variations. From left to right, viewpoints are the base unmodified path, offset left 2m, offset right 2m, angled up $30\degree$, and angled right $30\degree$}
%    \label{fig:dataset-example}
%\end{figure}

\subsection{Place Recognition: Sum of Absolute Differences}

The first set of tests evaluated a simple Sum-of-Absolute-Differences (SAD) based place recognition approach. While this is not a state-of-the-art technique in and of itself, it has formed the foundation of major place recognition algorithms including RatSLAM \cite{milford2010persistent} and SeqSLAM. 

Before matching, each image is down-sampled to 64x64 pixels and reduced to greyscale, as per previous usage. The matcher used compares each image in a query dataset to each of those in a reference dataset, and considers the reference image with the lowest sum of absolute difference to be a match. The performance measure consists of the percentage of images for which the matched reference image is taken from a place close to the query image.

The matcher is tested using 130 different combinations of time of day and viewpoint changes, so that we can see how the performance falls off as both increase. The results are summarized in Figure \ref{fig:sad-results-stacked}.

Figure \ref{fig:sad-results-stacked} shows that SAD-based matching seems to be relatively robust against small lateral offsets, with performance degrading beyond approximately 1m of lateral shift. There is similar falloff as time of day approaches sunrise or sunset, and seems to be relatively symmetrical. The small number of samples makes this change seem relatively smooth, but it also seems plausible that it may instead change rapidly around dawn and sunset when the lighting change has the most effect. Resolving this is simply a matter of sampling the distribution further.

Interestingly, matching rate falls off similarly with angle change irrespective of the direction of change (compare the lower two plots in Figure \ref{fig:sad-results-stacked}). Matching performance seems to follow an exponential decay with angle. It may also be worth investigating the effects of multiple orientation offsets combined together to see how they compound. Were it the aim of this paper, it would be relatively simple to generate additional required sample passes to properly evaluate; for space reasons, we leave that to future work.

\begin{figure}[ht]
    \centering
    \includegraphics[width=7cm]{SAD-results-stacked}
    \caption{Match percentage for Sum of Absolute Differences for, from top to bottom, lateral offset, vertical orientation change (pitch), and lateral orientation change (yaw). Each offset was tested across several times of day.}
    \label{fig:sad-results-stacked}
\end{figure}

%\begin{figure}[h]
%    \includegraphics[width=8cm]{SAD-matching-performance-over-lateral-offset-and-time-of-day}
%    \caption{Sum of absolute difference matching over both time of day and lateral offset change.}
%    \label{fig:sad-results-stacked}
%\end{figure}

%\begin{figure}[h]
%    \includegraphics[width=8cm]{SAD-matching-performance-over-vertical-orientation-change-and-time-of-day}
%    \caption{Sum of absolute difference match rate over both time of day and vertical orientation change}
%    \label{fig:sad-results-pitch}
%\end{figure}

%\begin{figure}[h]
%    \includegraphics[width=8cm]{SAD-matching-performance-over-lateral-orientation-change-and-time-of-day}
%    \caption{Sum of absolute difference match rate over both time of day and lateral offset change}
%    \label{fig:sad-results-yaw}
%\end{figure}

\subsection{Place Recognition: SeqSLAM}

The second place recognition algorithm we investigated was SeqSLAM, first described by Milford and Wyeth \cite{Milford2012}. SeqSLAM is a place recognition algorithm that searches for loop closures by attempting to match sequences of similar images. It measures image similarity using sum of absolute differences as analysed in the previous section, and then searches for spatially coherent sequences of local best matches \cite{Milford2012}.

To perform the test, we generated 130 traverses across 5 times of day and 26 different viewpoint variations from the street scene used in the other tests (Figure \ref{fig:dataset-example}). We chose the baseline pass at noon as the reference dataset, and compared it to all 130 of the other datasets (including itself). For each dataset we calculated the maximum F1 score, with the results are summarized in Figure \ref{fig:seqslam-results-stacked}.

The first immediately obvious (and unpredicted) result is the anomalous low performance values for a left 0.2m offset in the morning and at sunset, and the low performance across all times of day at a 10\degree vertical orientation change.  All of these changes are sudden and extreme, so the initial inclination is to sample around these points to see if there is a smooth or sudden decline. This analysis reveals that SeqSLAM performance degrades suddenly under these types of environmental changes. 

The other feature of note in the performance characteristics is the way the F1 score tends to plateau for small translational offsets or orientation changes. When the algorithm performs well, it seems to do so irrespective of the time of day. Strong condition invariance has been noted in previous work as a particular feature of SeqSLAM \cite{Milford2012}, which these results confirm. Note however that when the performance falls off due to lateral viewpoint change, it becomes less condition invariant, falling off further toward sunset and sunrise.

Using high-fidelity simulation has allowed us to explore the behaviour of SeqSLAM more comprehensively than ever before, and has revealed new details of its behaviour. This process could easily be repeated for an even wider range of environment permutations, identifying algorithm weakpoints and enabling future research improvements.

\begin{figure}[t]
    \centering
    \includegraphics[width=7cm]{SeqSLAM-results-stacked}
    \caption{SeqSLAM performance over, from top to bottom, lateral offset, vertical orientation change (pitch), and horizontal orientation change (yaw). Each viewpoint change is captured over 5 times of day to evaluate how performance changes over both viewpoint and condition change}
    \label{fig:seqslam-results-stacked}
\end{figure}

%\begin{figure}[t]
%    \includegraphics[width=8cm]{SeqSLAM-results-offset}
%    \caption{SeqSLAM fscore over both time of day and lateral offset change}
%    \label{fig:seqslam-results-offset}
%\end{figure}

%\begin{figure}[t]
%    \includegraphics[width=8cm]{SeqSLAM-results-pitch}
%    \caption{SeqSLAM fscore over both time of day and lateral offset change}
%    \label{fig:seqslam-results-pitch}
%\end{figure}

%\begin{figure}[t]
%    \includegraphics[width=8cm]{SeqSLAM-results-yaw}
%    \caption{SeqSLAM fscore over both time of day and lateral offset change}
%    \label{fig:seqslam-results-yaw}
%\end{figure}

\subsection{Visual SLAM: ORB-SLAM}

The SLAM algorithm tested was ORB-SLAM \cite{Montiel2015}. ORB-SLAM is a feature-based monocular SLAM system, that performs feature-based visual feature tracking, place recognition, mapping and loop closure using ORB features.

To test OrbSLAM, we again generated a variety of image datasets from the same street scene, with 5 different times of day, a baseline pass and 20 different viewpoint variations. Due to the mapping element of OrbSLAM, all datasets were made to start and end in the same place. To test the performance, each of the datasets was appended to the noon baseline dataset (acting as a reference pass), and camera trajectories were generated. These trajectories were then compared with the ground truth to calculate the Absolute Trajectory Error after aligning their scale. Results are summarised in Figure \ref{fig:orbslam-results-stacked}.

The observed performance of ORB-SLAM is patchy, with inconsistent errors with no clear correlation to viewpoint or condition change. Poor performance is obtained on some of the baseline datasets with no viewpoint change, such as the morning pass. On the other hand, the performance of the algorithm doesn't seem to depend on viewpoint or time of day at all, but on other factors not controlled in our data.

\begin{figure}[t]
    \centering
    \includegraphics[width=7cm]{orbslam-results-stacked}
   \caption{ORB-SLAM average trajectory error over, from top to bottom, lateral offset, vertical orientation change (pitch), and horizontal orientation change (yaw). Each viewpoint change is captured for 5 different times of day.}
    \label{fig:orbslam-results-stacked}
\end{figure}

%\begin{figure}[t]
%    \includegraphics[width=8cm]{orbslam-results-stacked}
%    \caption{OrbSLAM average trajectory error over both time of day and lateral offset change}
%    \label{fig:orbslam-results-offset}
%\end{figure}
%
%\begin{figure}[t]
%    \includegraphics[width=8cm]{orbslam-pitch-results}
%    \caption{OrbSLAM average trajectory error over both time of day and lateral offset change}
%    \label{fig:orbslam-results-pitch}
%\end{figure}
%
%\begin{figure}[t]
%    \includegraphics[width=8cm]{orbslam-yaw-results}
%    \caption{OrbSLAM average trajectory error over both time of day and lateral offset change}
%    \label{fig:orbslam-results-yaw}
%\end{figure}

\subsection{SeqSLAM parameter tuning}

The detailed analysis we have performed can be used to inform subsequent actions, including parameter tuning. To demonstrate this, we again tested the performance of Sum of Absolute Differences on the noon datasets across the range of lateral offsets shown in Figure \ref{fig:dataset-example} with a variety of different matching offset window parameters. SeqSLAM uses Sum of Absolute Differences combined with an offset window parameter that shifts the image pixels to achieve a better matching image. The results of this test can be seen in Figure \ref{fig:offset-matching-param-results}.

This data suggests that for distances up to 3.5m offset from the reference location, it is best to use the smaller 4 pixel window range. The use of an offset window shows clear improvement in performance, but larger windows introduce false positive matches.

\begin{figure}[t]
    \centering
    \includegraphics[height=4cm]{offset-matching-results}
    \caption{Sum of Absolute Difference matching rate as the maximum offset window is increased. This result helps us choose a maximum offset appropriate for a given lateral offset}
    \label{fig:offset-matching-param-results}
\end{figure}

\textbf{Benefits of Simulation:} The performance curves generated using simulated datasets for SeqSLAM show a peculiar behaviour for certain parameter values. This enables us to focus analysis of the algorithm on these unexpected performance cases. The corresponding input parameters of SeqSLAM which might have caused the unexpected intermittent performance drop as shown in the Figure \ref{fig:seqslam-results-stacked} could be the \emph{Offset Matching Range} and/or \emph{Patch Normalization} factor. We further analysed the performance drop for different times of day at $10\degree$ pitch. Figure \ref{fig:seqSLAM-diffMat} shows the SAD matrix comparison with ground truth, true positives and false positives marked. The difference matrices show that the poor performance is being introduced by multiple aliased matches, suggesting that this particular combination of environmental parameters increases the aliased nature of the environment.

\begin{figure*}[t]
\centering
\begin{tabular}[t]{ccccccccc}
    \includegraphics[scale=0.18]{1-down30} &
    \includegraphics[scale=0.18]{2-down15} &
    \includegraphics[scale=0.18]{3-down10} &
    \includegraphics[scale=0.18]{4-down5} &
    \includegraphics[scale=0.18]{5-base} & 
    \includegraphics[scale=0.18]{6-up5} &
    \includegraphics[scale=0.18]{7-up10} &
    \includegraphics[scale=0.18]{8-up15} &
    \includegraphics[scale=0.18]{9-up30} \\
    Down $30\degree$ & Down $15\degree$ & Down $10\degree$ & Down $5\degree$ &
    Baseline &
    Up $5\degree$ & Up $10\degree$ & Up $15\degree$ & Up $30\degree$

\end{tabular}
    \caption{The SAD matrix for comparison of Noon Baseline dataset with a change in orientation of camera's pitch. The ground truth is marked in blue and forms the diagonal of the matrix with true positives overlaid in yellow on the diagonal. The false positives are marked in red. The simulation results enable us to produce these specific confusion matrices, which clearly show that the abnormal performance is caused by increased environmental aliasing under these particular conditions, shown by the off diagonal false positive matches.}
    \label{fig:seqSLAM-diffMat}
\end{figure*}

\subsection{Comparison to real-world}

It is important to verify that performance change in simulation is representative of performance change in the real world. To test this, we compared performance drop over lateral viewpoint change using SeqSLAM for data from a real street and for data generated from a simulation of a similar street. The real data experiment images are shown in Figure \ref{fig:real-world-data} and the performance results can be seen in Figure \ref{fig:real-world-results}.

\begin{figure*}[t]
\centering
    \includegraphics[width=15cm,height=1.4cm]{real-data-lateralOffset} \\
    \vspace{0.1cm}
    \includegraphics[width=15cm,height=1.4cm]{simulated-data-lateralOffset}
    \caption{Images for five different traversals of a street with different lateral offsets. This real world data (top) is used to compare the trend of performance change with change in lateral shifts as compared to the simulated data (bottom).}
    \label{fig:real-world-data}
\end{figure*}

Unsurprisingly, observed performance is consistently better in simulation; we ascribe this to a lack of sufficient realism in the simulation, which was built simply and quickly. However, the performance follows the same trends in both environments, falling off smoothly as the offset difference increases. This demonstrates that performance results obtained in simulation obey similar trends to data obtained from even simplistic simulations.

\begin{figure}[t]
    \centering
    \includegraphics[height=5cm]{real-world-results}
    \caption{Comparison of SeqSLAM performance falloff between simulated and real-world environments as lateral offset increases. As is often the case, simulated performance is better in an absolute sense, but the trend is the same in both cases.}
    \label{fig:real-world-results}
\end{figure}

%\section{RESULTS}

% TODO: I don't think this section exists explicitly, I'm keeping it for notes
%contributions streams throughout the paper

%\begin{enumerate}
%    \item simulation as a useful tool
%    \item closing the loop - redoing based on automated triggers (not enough resolution)
%    \item methods for comparing different algorithms - given this tool, how compare, and what actions to take
%    \item ideal - some demonstrations of how to take action based on results of simulation
%    \begin{itemize}
%        \item Make some simple suggestions here, such as determining what ratio of training images are needed 
%    \end{itemize}
%\end{enumerate}

\section{Analysis of Object Recognition Viewpoint Dependency} \label{sec:object-recognition}

Another area of robotics research that can benefit from high fidelity simulation is visual object recognition and detection.
Much of the recent progress in this field has been driven by the computer vision community's intensive race to achieve ever improving performance on large image recognition datasets like ImageNet \cite{Russakovsky14} that is curated from large online photo repositories. State-of-the-art deep learning techniques now rival or surpass human performance and often approach perfect performance on these test sets. Rationally, related research fields such as robotics should be the beneficiary of such significant advances, but with a few exceptions, deep learning techniques have made little headway into the robotics field. Perhaps the primary reason for this disconnect is that robots operating in unpredictable, real world environments encounter visual imagery with very different characteristics and biases (or lack of biases) to that seen in traditional computer vision datasets. 


The high-fidelity simulation framework discussed in this paper allows to render realistic views of arbitrary objects from different viewpoints, under different lighting conditions, backgrounds, and occlusions. Such a dataset enables an in-depth analysis of current object recognition approaches to understand under which conditions and viewpoints good performance can be expected, and to inform and improve the training process to address the discovered challenges.

To demonstrate the potential of such analysis, we perform a viewpoint dependency test for an exemplary state-of-the-art convolutional network for object recognition. We use the simulation framework to generate views of a coffee mug object by moving the camera around the object on a sphere. We increment pitch and yaw in 5 degrees steps while keeping the camera pointed at the object centre. The generated images were then classified by the \texttt{vgg\_s} convolutional network \cite{Chatfield14}. Fig. \ref{fig:obj-recognition} illustrates the results and reveals the object-specific viewpoint dependency: The mug could only be correctly identified when the handle was clearly visible, sticking out to the side. Furthermore, even with the handle visible, the classification failed under insufficient lighting conditions (e.g. in the area around pitch 175, yaw 100). Interestingly the mug can be identified correctly if seen from above, i.e. looking inside the mug (pitch 175 degrees), but not when observing the mug from below. Such insights can help improve the training process of object classifiers and the simulation framework can even be used to generate more training data from viewpoints or under conditions that are hard to replicate in reality.

\begin{figure}[t]
    \centering
    \includegraphics[height=4cm]{object_recognition.pdf}
    \caption{Simulated viewpoint-dependency test for the \texttt{vgg\_s} convolutional network \cite{Chatfield14} on a coffee mug object. Successful recognition (shaded green) requires the mug handle to be clearly visible and the object to be well-illuminated.}
    \label{fig:obj-recognition}
\end{figure}


\section{DISCUSSION} \label{sec:discussion}

The most difficult and time consuming aspect of using high-fidelity simulation is creating the scene in the first place. Scenes are built of 3D models and textures, which are time consuming to create and arrange in the scene. The more realistic the scene requirement, the more labour is required. The creation of high quality 3D models and scenes is the speciality of a professional 3D artist, so if a particular problem requires large or realistic scene it may be necessary to hire a 3D artist to construct it. It can also be extremely difficult to maintain a consistent scale throughout the scene. Examination of our scene in Figure \ref{fig:street-scene-overview} will reveal scale flaws, note for instance the height of the houses relative to the width of the road.

However, once a particular simulation has been created, the payoff is huge as it can be used to generate arbitrary amounts of image data. Once we constructed the street scene and set up the required tools, capturing each dataset could be specified programatically and takes relatively little (human) time. Indeed, when we initially created the  datasets used to test the place recognition algorithms above, we tested orientation change at $30\degree$ and $15\degree$ only. However, after observing the way matching performance falls off with angle in Figure \ref{fig:sad-results-stacked}, we were able to very easily add tests for $5\degree$ and $10\degree$ orientation changes as well. The very fact that we can exactly repeat a movement through a scene with a precise orientation change is a powerful advantage of high-fidelity simulation.

It can in some sense be too easy to capture data. Since capturing additional data is often simply a matter of adding a new modifier to a path, increasing a sample range or decreasing a sample increment, it can be very easy to generate too much data. For instance, it may be tempting to sample a street scene such as ours at every offset up to 4m either side in 1m increments, and at each location take all vertical and horizontal changes up to $30\degree$ in $5\degree$ intervals. This is relatively simple to specify, but when multiplied out, produces $9 \times 13 \times 13 = 1521$ images per forward step down the path. The path we used with a step distance of 1m as well requires 612 forward steps, for a total therefore of 1965132 images. When generating datasets, we averaged about 6 frames per second, so generating this data would take approximately 91 hours. Further tweaking any of the specified numbers could multiply this number even further.

Rather than sampling densely with a large initial dataset, we recommend initial testing be done relatively sparsely over the test domain, with the initial results used to choose a second round of test values. For instance, given the distribution for vertical orientation change in Figure \ref{fig:sad-results-stacked}, it makes more sense to generate new test data at $2.5\degree$ and at $7.5\degree$ than at $25\degree$. In the future, it should be possible to automate this iterative testing, automatically choosing new test datasets based on the results of previous testing.

It is also important to note that the difficulty of a particular change to the simulation can be non-intuitive. For instance, it is very easy in the simulation to change the location or orientation of an object or the camera; or to change the base colour of a flat-coloured object. For this reason, it was very easy for us to add additional variations on the camera path, since this simply changes the camera's location and orientation. On the other hand, changing the lighting is simple manually, but for good quality lighting a lot of data needs to be recalculated, which takes a lot of time and is not designed to be triggered programatically. As such, it takes longer to generate data across different times of day, and it would be more time-consuming for us to test at an additional time of day.

\section{CONCLUSIONS AND FUTURE WORK} \label{sec:conclusion}

In this paper we investigated the use of photo-realistic gaming engine simulation tool to address important challenges in robotic vision across three example domains of place recognition, SLAM and object recognition: that
experiments cannot  be repeated exactly and that algorithms cannot be quantitatively compared.
The simulation allows us to create high realistic images from an arbitrary camera or cameras in a complex 3-dimensional world in which lighting and atmospheric
conditions can be fully controlled.
We  showed how we can systematically evaluate standard robotic vision algorithms (robust place recognition and visual SLAM) in ways which have not
been previously possible.  These algorithms are broadly representive of the classes of image-based and feature-based methods.
We also showed how we can synthetically generate a large number of images to characterise the performance of deep networks against viewpoint change. 

We have only just begun to scratch the surface of this new approach to robotic vision.
So far our paths through the environment are set manually rather than being driven by a robotic vision algorithm.
Integrating the game engine into the ROS environment would enable us to run a robotic vision algorithm in a manner analagous to a 
hardware-in-the-loop simulator, with its output commanding the camera pose in the simulator and the rendered image serving as input.
We also plan to extend our testing of simulation as a tool beyond the three domains presented here. 

Creating complex 3D worlds is time consuming and we are investigating modern 3D reconstruction techniques to create a first draft of a simulated world. It should also be possible to procedurally generate many parts of the environment, establishing rules for the generation of roads, cities, and other environments allowing variations to be produced quickly. Finally, many assets need only be created once, and as the community grows, the cost of setting up and using high fidelity simulations should progressively decrease. 

%from camera to simulation
%There remains a significant amount of applications that we have not yet explored. At this time, we have not successfully tested interaction between robot control code and high-fidelity simulation, further work is required on the code interfaces that make this possible. With further investigation, it should be possible to create full simulators for computer vision code including control code that interacts with the simulated world. This opens up the advantages of high-fidelity simulation to all computer vision algorithms, rather than only to passive algorithms.
%
%This paper has focused on using simulation to test and analyse computer vision algorithms, but the generation of image data can also be used to train learning algorithms, such as deep-learning neural nets. Interestingly, it should be possible to apply the iterative testing approach outlined here to network training, alternating between generating new test data to fill holes in the performance graph and generating new training data to correct areas of poor performance, with a reduced risk of over-training.
%%
%The key problem with using high-fidelity simulation is that generating simulation environments is too costly. There are a number of different ways that this can be made easier. Firstly, it is often desirable for simulation environments to mirror a real-world environment in some way, and so it should be possible to use modern 3D reconstruction techniques to lay the groundwork for a simulation environment. It should also be possible to procedurally generate more about the environment, establishing rules for the generation of roads, cities, and other environments allowing variations to be produced quickly. Finally, many assets need only be created, and a wider community using simulation and sharing work should reduce the initial costs of setting up a simulation.
%=======
%\subsection{Future Work}
%
%There remains a significant amount of applications that we have not yet explored. At this time, we have not successfully tested interaction between robot control code and high-fidelity simulation, further work is required on the code interfaces that make this possible. With further investigation, it should be possible to create full simulators for computer vision code including control code that interacts with the simulated world. This opens up the advantages of high-fidelity simulation to all computer vision algorithms, rather than only to passive algorithms.
%
%This paper has focused on using simulation to test and analyse computer vision algorithms, but the generation of image data can also be used to train learning algorithms, such as deep-learning neural nets. Interestingly, it should be possible to apply the iterative testing approach outlined here to network training, alternating between generating new test data to fill holes in the performance graph and generating new training data to correct areas of poor performance, with a reduced risk of over-training.
%
%The key problem with using high-fidelity simulation is that generating simulation environments is too costly. There are a number of different ways that this can be made easier. Firstly, it is often desirable for simulation environments to mirror a real-world environment in some way, and so it should be possible to use modern 3D reconstruction techniques to lay the groundwork for a simulation environment. It should also be possible to procedurally generate more about the environment, establishing rules for the generation of roads, cities, and other environments allowing variations to be produced quickly. Finally, many assets need only be created, and a wider community using simulation and sharing work should reduce the initial costs of setting up a simulation.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\addtolength{\textheight}{-0.22cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section*{APPENDIX}

%Appendixes should appear before the acknowledgment.

%\section*{ACKNOWLEDGMENT}

% Who should I thank here? The institute? thanking a co-author doesn't seem right. Epic Games, since it's their program I'm using?

% The preferred spelling of the word "acknowledgment" in America is without an "e" after the "g". Avoid the stilted expression, "One of us (R. B. G.) thanks . . ."  Instead, try "R. B. G. thanks". Put sponsor acknowledgments in the unnumbered footnote on the first page.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\bibliographystyle{plain}
\bibliography{john-bibliography,sourav-visual-SLAM,HFSinVision}  



\end{document}
