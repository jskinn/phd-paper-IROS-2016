% !TEX root = root.tex
Robotic vision involves processing a stream of images from a camera attached to a robot moving through a physical environment.
The pose of the robot's camera is controlled and is a function of previous images.  The environment typically has a complex 3D structure with transient 
distractor objects of unknown type and motion, as well as time varying lighting conditions.
An important consequence is that no robot vision experiment can ever be repeated.
Nor can the performance of different algorithms be compared, as they are in computer vision research, since they will be evaluated under different conditions: the robot initial conditon may vary, the lighting may vary, and the camera path which is a function of previous images may vary.

In contrast,  progress in computer vision research is driven by datasets of images which are are static, not temporally related and there is no scope for the algorithm to
request a slightly different view of the scene as is possible in robotic vision.
Computer vision research has recently made giant strides in performance through the use of deep learning, but this is fundamentally limited by the amount of representative real world image data could be captured and labelled for  training and testing purposes. 

In this paper we propose the use of a state-of-the-art photo-realistic simulation tool, the Unreal Engine 4 by Epic Games, to address these challenges.
This tool, developed for gaming,  allows the creation of complex 3-dimensional worlds that are realistically rendered.  The  view from a camera at any arbitrary pose can be obtained and this allows us to mimic the motion of a robot through the environment, and the robot can one or many cameras. We can also change the illumination conditions, adjusting the position of the sun, the cloud conditions, artificial light sources and atmospheric conditions such as fog.
A particular advantage is that ground truth is known, camera poses and object positions estimated by robotic vision algorithms can be compared against the
simulation state. 

The key contribution of this paper is studying the efficacy of this new tool for robotic vision, in particular evaluating algorithms in a systematic and repeatable manner, and generating synthetic data for training deep networks.
We test our ideas for three common robotic use cases.
Firstly, robust place recognition using the SeqSLAM\cite{Milford2012} algorithm.  We capture a reference path through the world at a particular time of day and then evaluate precision-recall (which we summarise as F1 score) for different paths through the world and for different lighting conditions.  The simulation allows us to change the path and the lighting independently -- something that cannot be done when capturing image sequences from the real world.
Secondly, for visual SLAM we evaluate the performance of OrbSLAM in an environment where the ground truth is known (the simulation model and camera path) and the camera path and lighting conditions are varied.
Thirdly, we look at object recognition.  We use the simulator to exhaustively render every combination of pitch and yaw in small increments. We then use this data to evaluate the viewpoint independence of an exemplary state-of-the-art convolutional network.

The next section presents prior work in the area and introduces the tools that we use.  Sections \ref{sec:analysis-place-rec} - \ref{sec:object-recognition} evaluate our approach for robust place recognition,
visual SLAM and object recognition.  Finally Section \ref{sec:conclusion} presents our conclusions and future work. 

%Computer vision research typically relies on image datasets collected from the real world to test, train and analyse the algorithms developed. But acquiring the necessary image data from the real world can be difficult, expensive or, for certain kinds of data, outright impossible. For instance, research on algorithms that desire to be viewpoint invariant or condition invariant requires images from a wide variety of viewpoints or a variety of conditions. 
%
%Another key limitation on real-world image datasets is a lack of repeatability. Usually each experimental run or dataset capture is unique, as changes in time of day or time of year or weather all change lighting conditions, sky and, background elements. Even the exact position of the camera will vary at the small scale, shifting pixels and producing similar but distinct images, even in the most static environments.
%
%It is also often difficult to capture a sufficient amount of data from the real world. When working with supervised learning algorithms, such as modern deep-learning neural nets, a large amount of data is required to train and test the network. Crucially, this data must be of sufficient variety that the resulting network can generalize successfully to unseen data rather than over-fitting to the training data.
%
%Further, a reliance on limited real-world datasets in turn limits the ability to demonstrate the generality and limitations of an algorithm. Take for instance the demonstrations of depth invariance in SMART \cite{pepperell2015automatic}, in which the adjusted SMART algorithm is shown to be capable of identifying places given a 4-lane change in depth. No claims are or can be made about the limitations of this technique; over how large a depth-change can the algorithm maintain performance? 100m? 1 km? How does the performance fall off as the depth increases? We cannot answer these questions using real-world data, as collecting the requisite images would be impossible.
%
%In this paper, we demonstrate that many of these issues can be addressed by generating image data on demand from a high-fidelity simulation environment. High-fidelity simulation is capable of generating arbitrary amounts of image data, which can be used to both train and test computer vision algorithms.