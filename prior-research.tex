% !TEX root = root.tex

\subsection{Simulation}

The use of simulation in robotics is not in of itself a new idea. Popular robot simulators such as Gazebo \cite{Koenig2004} and Player/Stage \cite{Gerkey2003} have existed for many years, and are commonly used to test robot motion and control. The use of game engines for robot simulation is also not new, with the USARsim project based on the Unreal Tournament 3 engine \cite{Carpin2007}, and other projects using the Unity engine \cite{mattingly2012robot}. More modern, photo-realistic engines such as Unity 5 or Unreal Engine 4 (the successor to the Unreal Tournament 3 engine) are yet to be adopted widely by the research community, despite their significant advantages over previous generation technology.

Most robotic vision research has used image datasets captured from the real world. This is no doubt in part due to a natural skepticism of "`simulation"' results - the key requirement for simulation in computer vision is that the simulator is capable of photo-realistic rendering and lighting, which has historically been out of reach for older platforms, including the Unreal Tournament 3 engine or Gazebo. Unreal Engine 4 however has powerful tools for realistic materials and lighting \cite{karis2013real}, which make it possible to move a camera through a simulated environment and produce images similar to those obtained in the real world.

Unreal Engine 4 has a number of additional advantages that make it suitable as a simulator platform for computer vision. It is developed and maintained by Epic Games Inc, and uses physics and modelling tools from nVidia, which allow for complex and interactive dynamic environments. It also allows full access to its source code, allowing a simulation designer complete control over the simulation. It is also free for non-commercial uses, including research - an important property if it is to be widely adopted by the research community.

\subsection{Place Recognition and Visual SLAM}
A place is defined as a distinct 2D or 3D location in an environment. In robotic vision, visual places are described using the image features which can be broadly classified into local and global image descriptors. The most common and efficient approaches for recognizing a place revisited by a robot make use of Bag of Words approach with features, for example, SURF in FAB-MAP \cite{Cummins2010} and most recently ORB in ORB-SLAM \cite{Montiel2015} etc. Some extensions of such methods also include building vocabulary online in an incremental fashion or incorporating geometric constraints between words for better performance. These methodologies allow a wide baseline matching of places, but they are brittle towards vast changes in appearance of the environment. On the other hand, use of global image descriptors like BRIEF-GIST \cite{Sunderhauf2011} or patch-normalized downsampled images as in SeqSLAM \cite{Milford2012} allows matching across change in conditions, but lacks robustness towards viewpoint variations. There are some place recognition methods which have proven to work well with both condition and viewpoint variations as in \cite{McManus2014}, \cite{Milford2008} and \cite{Niko2015}. Some of the methods describe places in 3D using only monocular camera by employing Structure from Motion (SfM) techniques. This helps in sparse \cite{Montiel2015}, semi-dense \cite{Engel2014lsd, Mur-Artal2015b} or dense \cite{Newcombe2011} reconstruction of environment for visual SLAM and other similar applications.
\\
The main challenges in place recognition lie in obtaining simultaneous robustness towards both change in conditions and viewpoint of a place. The environments with bland and texture-less images, motion blur, and effects introduced by camera properties like rolling shutter, granular noise etc. make it even more challenging to develop a high performance place recognition algorithm.
\\
Visual SLAM systems typicallycomprise of a place recognition, visual odometry and mapping backend component. The overall challenges for such systems are related to consistently calculating camera motion between keyframes/frames, relocalizing camera position when tracking fails, handling dynamic changes in the environment, performing efficient loop closures to get rid of scale drift in the map, and efficient 3D map construction.
\\
A robust place recognition or SLAM system needs to be evaluated against all the challenges mentioned above for a complete analysis of its performance. Such an in-depth analysis is often limited by a lack of variety in existing experimental datasets, or bias towards choosing datasets that work for the assumptions made by the particular algorithm. The existence of a high fidelity, fully controllable simulated environment provides a tool for overcoming these limitations and biases by performing thorough performance analyses.
\\
There have been some past attempts towards generating simulated environments for evaluating robotic vision algorithms. Handa et. al. in \cite{handa2014benchmark} developed 3D models for living room and office room, and gathered camera trajectories for benchmarking RGB-D data based visual odometry, 3D reconstruction and SLAM algorithms. Peris et. al. in \cite{peris2012towards} created a simulated dataset for stereo systems for different illumination conditions. The work was inspired by the lack of adequate ground truth, especially the disparity maps for stereo vision. Similarly, Butler et. al. in \cite{butler2012naturalistic} rendered images from an animated movie \emph{Sintel} to create a dataset for evaluating optical flow methods. Ravi et. al. in \cite{rathnam2013initial} used a high fidelity marine simulator to test distributed cooperative 3D exploration algorithm for AUVs. Most of the work done in this regard has been focused on specific applications.