% !TEX root = root.tex

\subsection{Simulation}

The use of simulation in robotics is not a new idea. Popular robot simulators such as Gazebo \cite{Koenig2004} and Player/Stage \cite{Gerkey2003} have existed for many years, and are commonly used to test robot motion and control. The use of game engines for robot simulation is also not new, with the USARsim project based on the Unreal Tournament 3 engine \cite{Carpin2007}, and other projects using the Unity engine \cite{mattingly2012robot}. Nobody has yet made use of modern, high-fidelity engines such as Unity 5 or Unreal Engine 4 (the successor to the Unreal Tournament 3 engine).

The more specific field of robotic vision has not seen much application of simulation at all, it instead prefers to work from image datasets captured from the real world.  The key requirement for simulation in computer vision is that the simulator is capable of photo-realistic rendering and lighting, which has historically been out of reach for older platforms, including the Unreal Tournament 3 engine or Gazebo. Unreal Engine 4 however has powerful tools for realistic materials and lighting \cite{karis2013real}, which make it possible to create simulated environment that produce images similar to the real world.

Unreal Engine 4 has a number of additional advantages that make it suitable as a simulator platform for computer vision. It is developed and maintained by Epic Games Inc, and uses physics and modelling tools from nVidia, which allow for complex and interactive dynamic environments. It also allows full access to it's source code, allowing a stimulation designer complete control over the simulation. It is also free for non-commercial uses, including research.

\subsection{Place Recognition}
A place is defined as a distinct 2D or 3D location in the representation map of environment. In robotic vision, visual places are described using the image features which can be broadly classified into local and global image descriptors. The most common and efficient approaches for recognizing a place revisited by a robot make use of Bag of Words approach with features, for example, SURF in FAB-MAP \cite{Cummins2010} and ORB in ORB-SLAM \cite{Montiel2015} etc. Some extensions of such methods also include building vocabulary online in an incremental fashion or incorporating geometric constraints between words for better performance. These methodologies allow a wide baseline matching of places, but they are brittle towards vast changes in appearance of the environment. On the other hand, use of global image descriptors like BRIEF-GIST \cite{Sunderhauf2011} or patch-normalized downsampled images as in SeqSLAM \cite{Milford2012} allows matching across change in conditions, but lacks robustness towards viewpoint variations. There are some place recognition methods which have proven to work well with both condition and viewpoint variations as in \cite{McManus2014}, \cite{Milford2008} and \cite{Niko2015}. Some of the methods describe places in 3D using only monocular camera by employing Structure from Motion (SfM) techniques. This helps in sparse \cite{Montiel2015}, semi-dense \cite{Engel2014lsd, Mur-Artal2015b} or dense \cite{Newcombe2011} reconstruction of environment for visual SLAM and other similar applications.
 
\begin{itemize}
    \item Sequence SLAM
    \begin{itemize}
        \item Developed by Milford and Wyeth... (citation)
        \item Performs place recognition using 
    \end{itemize}
    \item {Challenges}
        \begin{itemize}
            \item challenges include viewpoint invariant place recognition, condition invariant place recognition, place recognition in dynamic environments
            \item 
        \end{itemize}
\end{itemize}